57	This step applies the <b>Brown's Double Exponential Smoothing</b> algorithm to denoise the input time series data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>A real data smoothing parameter between 0 and 1</li></ol>
58	This step applies the well-known <b>Fast Fourier Transform</b> algorithm to decompose a signal into its frequency components. The signal noise can be removed by suppressing the high frequency components.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>The cut-off frequency</li></ol>
56	This step applies the <b>Holt-Winters Double Exponential Smoothing</b> algorithm to denoise the input time series data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>A real data smoothing parameter between 0 and 1</li><li>A real trend smoothing parameter between 0 and 1</li></ol>
59	This step applies the <b>LOWESS</b> algorithm to denoise the input time series data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li></ol>
61		<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li></ol>
45	The step applies the <b>Centered Moving Average</b> algorithm to denoise one-dimensional time series data. Note that the algorithm does not need the timestamp information to denoise the data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li></ol>
46	The step applies the <b> Forward Moving Average </b> algorithm to denoise one-dimensional time series data. Note that the algorithm does not need the timestamp information to denoise the data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li></ol>
51	The step applies the <b> Backward Moving Median </b> algorithm to denoise one-dimensional time series data. Note that the algorithm does not need the timestamp information to denoise the data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li></ol>
50	The step applies the <b> Centered Moving Median </b> algorithm to denoise one-dimensional time series data. Note that the algorithm does not need the timestamp information to denoise the data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li></ol>
52	The step applies the <b> Forward Moving Median </b> algorithm to denoise one-dimensional time series data. Note that the algorithm does not need the timestamp information to denoise the data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li></ol>
53	The step applies the <b> Percentile Filter </b> algorithm to denoise one-dimensional time series data. Note that the algorithm does not need the timestamp information to denoise the data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li><li>A specified percentile of a certain number of data points around the considered data point</li></ol>
60		<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li></ol>
54	This step applies the <b> Savitzky-Golay </b> algorithm (a moving polynomial fit) to denoise data.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li><li>The polynomial order</li></ol>
55	This step smooths noisy time series data by assigning exponentially decreasing weights over time to each individual value.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>A real data smoothing parameter between 0 and 1</li></ol>
48	This step denoises data similarly to the pre-processing step based on the <b> Backward Moving Average </b> algorithm, but additionally gives a weighting factor to each value within the filter width. Note that the applied algorithm doesn't need the timestamp information for the denoising procedure.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li><li>Weights for values in the filter width</li></ol>
47	This step denoises data similarly to the pre-processing step based on the <b> Centered Moving Average </b> algorithm, but additionally gives a weighting factor to each value within the filter width. Note that the applied algorithm doesn't need the timestamp information for the denoising procedure.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li><li>Weights for values in the filter width</li></ol>
49	This step denoises data similarly to the pre-processing step based on the <b> Forward Moving Average </b> algorithm, but additionally gives a weighting factor to each value within the filter width. Note that the applied algorithm doesn't need the timestamp information for the denoising procedure.	<ol><li>Denoised time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Filter width</li><li>Weights for values in the filter width</li></ol>
64	This step deletes all data tuples containing improper values. It is widely used in many statistical packages.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li></ol>
76	This step applies the <b>C4.5</b> algorithm to reconstruct any improper values.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li></ol>
66	This step replaces any improper values in the data vector with the value immediately preceding it.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li></ol>
74	This step first estimates the parameters of the linear model using the data. Then, the linear model with known parameters can reconstruct improper values at corresponding times.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li></ol>
68	This step replaces any improper values in the data vector with the <b> Mean </b> of the neighbourhood values.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li><li>Local window size</li></ol>
71	This step replaces any improper values in the data vector with the <b> Median </b> of the neighbourhood values.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li><li>Local window size</li></ol>
72	This step replaces any improper values in the data vector with the <b> Midrange </b> of the neighbourhood values.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li><li>Local window size</li></ol>
73	This step replaces any improper values in the data vector with the <b> Mode </b> of the neighbourhood values.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values </li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li><li>Local window size</li></ol>
75	This step first estimates the parameters of the polynomial model using the data. Then, the polynomial model with known parameters can reconstruct improper values at corresponding times.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li></ol>
69	This step replaces any improper values in the data vector with the <b> Trimmed Mean </b> of the neighbourhood values and is resistant to outliers.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li><li>Percentage of high and low extremes to be trimmed</li><li>Local window size</li></ol>
67	This step uses the reference data to reconstruct improper values. An improper value $$Y_i$$ at time step $$i$$ is estimated using the following formula: <p>$$\hat{Y}_i = \frac{Y_{i-1}}{Y^r_{i-1}} Y^r_i$$,</p> where $$\hat{Y}_i$$ is the reconstructed value at time step $$i$$ and $$Y^r_i$$ is the reference data value at time step $$i$$.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li></ol>
70	This step replaces any improper values in the data vector with the <b> Weighted Mean </b> of the neighbourhood values.	<ol><li>Output data excluding improper values</li></ol>	<ol><li>Input data including improper values</li><li>Definition of improper values ('nan', 'inf', or 'null', etc.)</li><li>Weights for all available values</li><li>Local window size</li></ol>
39	This step detects outliers in a data set by applying the <b>Box Plot</b> technique.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
43	"This step detects outliers in multivariate data by applying the modified version of <b>Chi-Squared Test</b>. The basic formula is as follows: <p>$$\chi^2 =  \sum_{i=1}^{N} \frac{(o_{i} - E_i)^2}{E_i}$$,</p> where $$o$$ is the object to be tested and $$o_i$$ is the value of $$o$$ on the $$i$$th dimension. $$E_i$$ is the mean value on the $$i$$th dimension among all objects. The object may be identified as an outlier if the Chi-value is larger than a threshold value."	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Multivariate data including outliers</li></ol>
37	This step detects outliers by comparing a ratio of ranges to an established table of values to determine whether the value in question is an outlier.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
35	This step is ued to detect a single outlier in a univariate data set by applying the <b> Grubbs' Test </b>.	<ol></ol>	<ol></ol>
34	This step detects outliers in a data set by applying the <b> Hampel Identifier </b> algorithm.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
42	This step first partitions the data points into $$k$$ clusters by applying the <b>K-Means Clustering</b> algorithm. Then, a distance ratio <p>$$\frac{dist(Y_o, c_o)}{\bar{c}_o}$$ </p> for each data object is calculated, where $$Y_o$$ is the data object, $$c_{o}$$ is the center of the cluster which $$Y_o$$ belongs to and $$\bar{c}_o$$ stands for the average distance between all data objects in that cluster and the center $$c_{o}$$. The larger the ratio, the farther away the data object is relative from the center. Finally, if the calculated ratio is above a pre-defined threshold, the observed data object is identified as an outlier.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Data samples abstracted in a n-dimensional feature space</li><li>Specified number of clusters</li><li>A pre-defined threshold for the distance ratios</li></ol>
33	This step detects outliers in a data set by checking the value in question's distance from the mean distribution.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
40	This step detects outliers in a data set by applying the <b>One-sided Median</b> algorithm.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
36	This step is ued to detect multiple outliers in a univariate data set by applying the <b> Tietjen-Moore Test </b>. 	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
38	This step applies the <b>Trimmed Mean</b> algorithm to identify outliers in the data.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
41	This step detects outliers in a data set by applying the <b>Two-sided Median</b> algorithm.	<ol><li>Original data with outliers marked</li></ol>	<ol><li>Input data including outliers</li></ol>
9	his step applies the <b> One-Rule Discretizer (1RD) </b> to discretize the data.	<ol><li>Discretized data</li></ol>	<ol><li>Input data</li></ol>
7	In this step, <b> CAIM </b> algorithm, which maximizes the class-attribute interdependence and generates a (possibly) minimal number of discrete intervals, is used to discretize the data. A priori knowledge on the number of intervals is not required.	<ol><li>Discretized data</li></ol>	<ol><li>Input data</li></ol>
8	This step first preforms an initial discretization for the input discrete data, then repeats a bottom-up merging process continuously until a termination condition is fulfilled. The merging process consists of two steps: (1) perform the Chi-squared test for each pair of adjacent intervals, (2) merge the pair of adjacent intervals with the lowest Chi-square value. Merging continues until all pairs. For details refer to the <a href ='http://sci2s.ugr.es/keel/pdf/algorithm/congreso/1992-Kerber-ChimErge-AAAI92.pdf'> ChiMerge algorithm</a>.	<ol><li>Discretized data</li></ol>	<ol><li>Input data</li><li>Chi-squared significance threshold</li></ol>
5	This step applies the <b> Equal-frequency Interval Binning </b> algorithm to discretize a data set contaning a very large number of values.	<ol><li>Discretized data</li></ol>	<ol><li>Input data</li><li>Frequency</li></ol>
4	his step applies the <b> Equal-width Interval Binning </b> algorithm to discretize the continuous Data or a data set contaning a very large number of values.	<ol><li>Discretized data</li></ol>	<ol><li>Input data</li><li>Interval width</li></ol>
6	<b> K-Means Clustering </b> algorithm is first used to partition the input data values into clusters. Then, the discretization strategy for the input data is made using the information of maximum and minimum values of the data set, computed cluster centers and midpoints between each two clusters.	<ol><li>Discretized data</li></ol>	<ol><li>Input data</li></ol>
86	Redundancy detection is an important task in data integration. This step applies the <b>Chi-Squared Test</b> to evaluate the correlation between two attributes (for nominal data). A very high Chi-value indicates that one attribute strongly implies the other and may be removed as a redundancy.	<ol><li>Redundant attributes</li></ol>	<ol><li>Nominal data</li></ol>
80	"This step is a conflict ignoring strategy that creates all possible value combinations and gives the user the choice among all possible values. For details refer to <a href ='http://www.vldb2005.org/program/paper/fri/p970-burdick.pdf'>Burdick, Deshpande and Jayram, 2005</a>."	<ol><li>Output data with conflict values</li><li>All found conflicting values</li><li>All possible values for resolving the conflict</li></ol>	<ol><li>Input data sources</li></ol>
84	This step is regarded as a conflict resolution strategy and uses the most recent value to resolve the data value conflict.	<ol><li>Output data without conflict values</li><li>All found conflict values</li></ol>	<ol><li>Input data sources</li></ol>
83	This step is regarded as a conflict resolution strategy and takes the average of all present values to resolve the data value conflict.	<ol><li>Output data without conflict values</li><li>All found conflict values</li></ol>	<ol><li>Input data sources</li></ol>
81	This step is regarded as a conflict resolution strategy and chooses the most common value among the conflicting ones to resolve the data value conflict.	<ol><li>Output data without conflict values</li><li>All found conflict values</li></ol>	<ol><li>Input data sources</li></ol>
79	"This step is a conflict ignoring strategy that passes all conflict values on to the users.The step does not tackle conflicts and let the users make the decision strategy, and thus may produce inconsistent results."	<ol><li>Output data with conflict values</li><li>All found conflicting values</li></ol>	<ol><li>Input data sources</li></ol>
82	This step is regarded as a conflict resolution strategy and picks one at random from all conflicting values to resolve the data value conflict.	<ol><li>Output data without conflict values</li><li>All found conflicting values</li></ol>	<ol><li>Input data sources</li></ol>
109	In this step the data to be pre-processed is first stored entirely. Then a separate process selects the non-duplicate elements by comparison and stores them in a new block.	<ol><li>A list of data elements excluding duplications</li></ol>	<ol><li>A list of data elements</li></ol>
100	This step applies the <b> K-Means Clustering </b> algorithm to reduce the size of input data set without losing much information.	<ol><li>Reduced data set</li></ol>	<ol><li>Large size of input data</li><li>Specified number of clusters</li></ol>
99	This step applies the <b> Principal Component Analysis </b> algorithm to search for orthogonal bases that best represent the data. Using these orthogonal bases, the original data can be projected onto a much smaller space. The size of the original data can be significantly reduced in this way.	<ol><li>Reduced data set</li></ol>	<ol><li>Normalized input data (large size)</li></ol>
105	This step applies the <b> Equal-frequency Interval Binning </b> algorithm to approximate the input data set and reduce its size.	<ol><li>Reduced data set</li></ol>	<ol><li>Large size of input data</li></ol>
104	This step applies the <b> Equal-width Interval Binning </b> to approximate the input data set and reduce its size.	<ol><li>Reduced data set</li></ol>	<ol><li>Large size of input data</li></ol>
102	This step first estimates the parameters of the linear model using the data, then replaces original data by smaller forms of data representation, namely to store only the estimated data parameters.	<ol><li>Reduced data set</li></ol>	<ol><li>Large size of input data</li></ol>
103	This step first estimates the parameters of the polynomial model using the data, then replaces original data by smaller forms of data representation, namely to store only the estimated data parameters.	<ol><li>Reduced data set</li></ol>	<ol><li>Large size of input data</li></ol>
107	This step applies the SRSWR algorithm to sample the input data set randomly. The size of original data set is then reduced.	<ol><li>Reduced data set</li></ol>	<ol><li>Large size of input data</li></ol>
106	This step applies the SRSWOR algorithm to sample the input data set randomly. The size of original data set is then reduced.	<ol><li>Reduced data set</li></ol>	<ol><li>Large size of input data</li></ol>
89	This step applies the <b> Decimal Scaling </b> algorithm to normalize data.	<ol><li>Normalized one-dimensional or multi-dimensional data</li></ol>	<ol><li>One-dimensional or multi-dimensional data</li><li>An integer scaling factor</li></ol>
91	This step applies the <b> Min-Max Scaling </b> algorithm to normalize data.	<ol><li>Normalized one-dimensional or multi-dimensional data</li></ol>	<ol><li>One-dimensional or multi-dimensional data</li><li>User-chosen minimum</li><li>User-chosen maximum</li></ol>
92	This step applies the <b> Unitnorm Scaling </b> algorithm to normalize data.	<ol><li>Normalized one-dimensional or multi-dimensional data</li></ol>	<ol><li>One-dimensional or multi-dimensional data</li></ol>
90	This step normalizes data based on the mean and standard deviation of the data set.	<ol><li>Normalized one-dimensional or multi-dimensional data</li></ol>	<ol><li>One-dimensional or multi-dimensional data</li><li>Mean value of input data</li><li>Standard deviation of input data</li></ol>
94	This step applies the <b> Bubble Sort </b> algorithm to sort a list of data elements in ascending or descending order.	<ol><li>A list of data elements in ascending order or descending order</li></ol>	<ol><li>A list of data elements</li></ol>
96	This step applies the <b> Quick Sort </b> algorithm to sort a list of data elements in ascending or descending order.	<ol><li>A list of data elements in ascending order or descending order</li></ol>	<ol><li>A list of data elements</li></ol>
95	This step applies the <b> Selection Sort </b> algorithm to sort a list of data elements in ascending or descending order.	<ol><li>A list of data elements in ascending order or descending order</li></ol>	<ol><li>A list of data elements</li></ol>
19	This step calculates the cumulative sum of the input data set using a reference value.	<ol><li>CUSUM values of the time series data</li></ol>	<ol><li>One-dimensional time series data</li><li>Reference value</li></ol>
14	This step applies the <b> Box-Cox Transformation </b> algorithm to make non-normally distributed data normal.	<ol><li>Time-series data that has approximately normal distribution</li></ol>	<ol><li>Non-normally time-series data</li></ol>
16	This step applies the <b> Box-Cox Transformation </b> algorithm to stabilze the variance in the data.	<ol><li>Output time-series data</li></ol>	<ol><li>Input time-series data</li></ol>
18	This step applies the <b> Sign Test </b> algorithm to test whether a time-series is nonstationary in variance.	<ol><li>Input time-series data</li><li>Test result: stationary or nonstationary</li></ol>	<ol><li>Input time-series data</li></ol>
17	This step applies the <b> Runs Test </b> algorithm to test whether a time-series is nonstationary in variance.	<ol><li>Input time-series data</li><li>Test result: stationary or nonstationary</li></ol>	<ol><li>Input time-series data</li></ol>
20	This step calculates the <b>Mean</b> value of the input data.	<ol><li>Mean value of the input data</li></ol>	<ol><li>Input data</li></ol>
21	This step calculates the <b>Median</b> value of the input data.	<ol><li>Median value of the input data</li></ol>	<ol><li>Input data</li></ol>
22	This step calculates the <b>Midrange</b> value of the input data.	<ol><li>Midrange value of the input data</li></ol>	<ol><li>Input data</li></ol>
23	This step calculates the <b>Mode</b> value of the input data.	<ol><li>Mode value of the input data</li></ol>	<ol><li>Input data</li></ol>
28	This step applies the <b>Backward Difference</b> method to approximate the first order derivative of a function or a time series. Note that step size(s) are known a priori for a time series.	<ol><li>First derivative of a function or a time series</li></ol>	<ol><li>A function or a time series</li><li>Step size(s)</li></ol>
29	This step applies the <b>Central Difference</b> method to approximate the first order derivative of a function or a time series. Note that step size(s) are known a priori for a time series. 	<ol><li>First derivative of a function or a time series</li></ol>	<ol><li>A function or a time series</li><li>Step size(s)</li></ol>
27	This step applies the <b>Forward Difference</b> method to approximate the first order derivative of a function or a time series. Note that step size(s) are known a priori for a time series.	<ol><li>First derivative of a function or a time series</li></ol>	<ol><li>A function or a time series</li><li> Step size(s)</li></ol>
30		<ol><li>Standard deviation of the input data sequence</li></ol>	<ol><li>Input data sequence</li></ol>
24	This step calculates the <b>Trimmed-mean</b> value of the input data.	<ol><li>Trimmed-mean value of the input data</li></ol>	<ol><li>Input data</li><li>Percentage of high and low extremes to be trimmed</li></ol>
12	"This step applies the Mean Absolute Deviation (MAD) algorithm to estimate the variance of the data. The basic formula is as follows: <p>$$\hat{\sigma} = K \cdot \text{MAD}$$,</p> where $$K \approx 1.4826$$, MAD is a measure returned by the Mean Absolute Deviation (MAD) algorithm and $$\hat{\sigma}$$ denotes the estimated variance."	<ol><li>Estimated variance</li></ol>	<ol><li>Input data</li></ol>
25	This step calculates the <b>Weighted-mean</b> value of the input data.	<ol><li>Weighted-mean value of the input data</li></ol>	<ol><li>Input data</li><li>Input weighting vector</li></ol>
